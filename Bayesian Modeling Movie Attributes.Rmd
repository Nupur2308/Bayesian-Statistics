---
title: "Bayesian Modeling Movie Attributes"
author: "Nupur G"
date: "2020-05-15" 
output: 
  html_document: 
    keep_md: yes
---
    

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
library(BAS)
library(tidyr)
library(cowplot)
library(ggplotify)
library(purrr)
```

### Load data


```{r load-data}
load("movies.Rdata")
```


* * *

## Part 1: Data 

The data is a random sample of 651 movies produced and released before 2016. Thus, hypothetically, it can be generalized to the population of American movies. However, a few factors may give rise to bias such as the fact that this dataset includes only English movies released in US theatres, Aditionally, let's look at the distribution of the theatrical release years below. As you can see, it is left skewed, with majority of the observations coming from the later years. Thus, we **have some concerns with generalizability**. 

Furthermore, since this is an observational study and there is **no random assigment, we cannot infer causality.** 

```{r}
hist(movies$thtr_rel_year,breaks=c(1970:2016))

```


* * *

## Part 2: Data manipulation

In this section we create new variables `feature_film`, `drama`, `mpaa_rating_R`, `oscar_season`, and `summer_season`. 

```{r}
movies2<-movies #copy to a new variable so that original data remains untouched 

movies2<-mutate(movies2, feature_film=ifelse(title_type=="Feature Film","yes","no"))%>%mutate(feature_film=as.factor(feature_film))
movies2<-mutate(movies2, drama=ifelse(genre=="Drama","yes","no"))%>%mutate(drama=as.factor(drama))
movies2<-mutate(movies2, mpaa_rating_R=ifelse(mpaa_rating=="R","yes","no"))%>%mutate(mpaa_rating_R=as.factor(mpaa_rating_R))
movies2<-mutate(movies2, oscar_season=ifelse(thtr_rel_month>=10,"yes","no"))%>%mutate(oscar_season=as.factor(oscar_season))
movies2<-mutate(movies2, summer_season=ifelse(thtr_rel_month %in% 5:8,"yes","no"))%>%mutate(summer_season=as.factor(summer_season))

str(movies2)
```

* * *

## Part 3: Exploratory data analysis 

Let's look at the new variables we created in the previous section and their relationship with audience score below. 

```{r fig.height=8, fig.width=10}
grep("audience_score",colnames(movies2))
movies_cat<-movies2[,c(18,33:37)]
movies_cat%>% gather(-audience_score, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = audience_score)) +
    geom_boxplot() +
    facet_wrap(~ var, scales = "free",ncol=3) +
    theme_bw()+
    ggtitle("Categoric variables plotted against audience_score")
```

We observe from the above boxplots that `feature_film`, `drama`, and `oscar_season` seem to have a relationship with `audience_score`. Specifically, **drama films, non-feature films and films released during oscar season seem to have a higher audience score.** An R MPAA Rating and release during summer months does not seem to have a relationship with audience score. 

Let's look at numerical summary statistics to look at exact differences. 

```{r}

movies_cat%>%filter(!is.na(audience_score))%>%group_by(feature_film)%>%summarize(mean_score=mean(audience_score,na.rm=TRUE),count=n(),median_score=median(audience_score,na.rm=TRUE))

movies_cat%>%filter(!is.na(audience_score))%>%group_by(drama)%>%summarize(mean_score=mean(audience_score,na.rm=TRUE),count=n(),median_score=median(audience_score,na.rm=TRUE))

movies_cat%>%filter(!is.na(audience_score))%>%group_by(mpaa_rating_R)%>%summarize(mean_score=mean(audience_score,na.rm=TRUE),count=n(),median_score=median(audience_score,na.rm=TRUE))

movies_cat%>%filter(!is.na(audience_score))%>%group_by(oscar_season)%>%summarize(mean_score=mean(audience_score,na.rm=TRUE),count=n(),median_score=median(audience_score,na.rm=TRUE))

movies_cat%>%filter(!is.na(audience_score))%>%group_by(summer_season)%>%summarize(mean_score=mean(audience_score,na.rm=TRUE),count=n(),median_score=median(audience_score,na.rm=TRUE))
```

So we can see from the above summary statistics that the mean and median audience scores of non-feature films, dramas, and films released during oscar seasons are higher. We will create a bayesian model in the next part to understand the posterior inclusion probabilities of these variables. 


* * *

## Part 4: Modeling

Our boss is interested in learning what attributes make a movie popular. She is also interested in learning something new about movies. She wants our team to figure it all out. Based on this, we can formulate the research question as follows: 

**Find the movie attributes that are related to movie popularity.**

In this section, we want to construct a model that will help us understand the attributes that are related to the popularity of a movie. Popularity of a movie is defined in terms of the `audience_score`. Since there are 2^16 models, we will use MCMC to find the posterior probabilities of the models and coefficients. The prior used for the model coefficients is 'BIC' which is implying that the prior distribution $$ \beta = (\beta_{0},\beta_{1},...)^T $$ is uniformly flat. 

```{r}
set.seed(1234)

bma_score <- bas.lm(audience_score ~ feature_film+drama+runtime+mpaa_rating_R+thtr_rel_year+oscar_season+summer_season+critics_score+imdb_rating+imdb_num_votes+best_pic_nom+best_pic_win+best_actor_win+best_actress_win+best_dir_win+top200_box, data = movies2,
                   prior = "BIC", 
                   modelprior = uniform(),method="MCMC")


summary(bma_score)
```

The above summary provides us the marginal posterior inclusion probabilities (pip) of each of the variables and then lists the variables included in the top 5 models. According to this summary, `imdb_rating` is including in every single model with a pip of 1. `critics_score` has the next highest pip of 0.89, followed by `runtime` with a pip of 0.47. The model with the highest posterior probability (0.1289) has the variables `imdb_rating`, `runtime`, and `critics_score`. 

`best_actor_win`, `best_actress_win` and `mpaa_rating_R` also have significant posterior inclusion probabilities. However, we should be careful of selecting the best model using these pips due to the presence of multicollinearity, as small posterior inclusion probability may arise when two or more variables are highly correlated. 

**Since even the best model has only posterior probability of 0.1289, we do not want to select any one model, but use Bayesian Model Averaging to determine coefficients and make predictions.**

### Model Coefficients

```{r fig.height=8, fig.width=10}

coef.score=coef(bma_score,estimator="BMA")

a<-round(confint(coef.score)[,1:2],6)
b<-round(coef.score$postmean,6)
c<-round(coef.score$postsd,6)

result<-data.frame(b,c,a)

colnames(result)<-c("Posterior mean","Posterior STD","2.5%","97.5%")

result
```


We can also graph the coefficients as follows: 

```{r}
plot(coef.score,ask=F)
```

**Findings:**

* We can see that 0 is included in the credible intervals of `feature_film`, `drama`, `oscar_season`, and `summer_season`, indicating that these variables do not conclusively relate to the audience score in one particular direction. 

* `imdb_rating`,`critics_score`, and `runtime` have the least uncertainty in their posterior coefficients and the highest pips as seen from the plots. In the plots of these variables, there is a noticeable peak and the line at 0 is not as high as the other explanatory variables. A 1 point increase in IMDB rating increases the audience score by 14.98 points on average. A 1 point increase `critics_score` drives up the audience rating by 0.06 on average. An additional minute of `runtime` drives down audience score by 0.03 on average. 

* The `mpaa_rating_R` variable has medium uncertainty in its coefficients as seen in the slight bulge in the posterior plot. An R rated film on average has an audience score of 0.3 lower than a non-R rated film. 

* A best picture nomination in `best_pic_nom` drives up audience score by 0.5 points on average, while the `best_pic_win` has an almost negligible relationship with audience score. The coefficients of `best_actor_win`, `best_actress_win` and `best_dir_win` are negative indicating an inverse relationship with audience score, but the credible intervals around these variables are wide indicating large uncertainty. 


Next, we shall conduct model diagnostics. 

### Model Diagnostics

```{r}
plot1<-diagnostics(bma_score, type = "model", col = "blue", pch = 16, cex = 1.5)
plot2<-diagnostics(bma_score, type = "pip", col = "blue", pch = 16, cex = 1.5)
plot(bma_score, which = 1, add.smooth = F, 
     ask = F, pch = 16, sub.caption="", caption="")
abline(a = 0, b = 0, col = "darkgrey", lwd = 2)
plot(bma_score, which=2, add.smooth = F, sub.caption="", caption="")
```


From the above diagnostic plots we are able to see the following: 

(A) **Convergence Plot: Posterior Model Probabilities**: Most points stay in the 45 degree diagonal line, meaing the posterior model probability from the MCMC method has mostly converged to the theoretical posterior model probability.

(B) **Convergence Plot: Posterior Inclusion Probabilities**: All the points are falling on the 45 degree diagonal, we can conclude that the posterior inclusion probability of each variable from MCMC have converged well enough to the theoretical posterior inclusion probability.

(C) **Residuals Versus Fitted Values Using BMA**: Unfortunately, the residuals show more variation for lower fitted audience scores, and less variation for higher fitted audience scores, specifically for scores above 80. Also, observations 126, 216 and 251 may be potential outliers. 

(D) **Cumulative Sampled Probability**: We can see that after we have discovered about 3,000 unique models with MCMC sampling, the probability is starting to level off, indicating that these additional models have very small probability and do not contribute substantially to the posterior distribution.

**Overall:** Overall, the model has converged as indicated by the convergence plots, but the residual plot raises concerns as the residuals do not have constant variance, indicating that some of the assumptions of linear relationship may be violated. 

* * *

## Part 5: Prediction

Let's take the movie: **The Magnificent Seven**

* Rotten Tomatoes: https://www.rottentomatoes.com/m/the_magnificent_seven_2016  
* IMDB: https://www.imdb.com/title/tt2404435/ 

We create a dataframe with all the data points to be fed to the predict function. 

```{r}

pred_data<-data.frame(matrix(nrow=0,ncol=16)) #create empty data frame

newData<-c("yes","no",133,"yes",2016,"no","no",6.9,178607,64,"no","no","no","no","no","no") #data points

pred_data<-rbind(pred_data,newData) #add row to empty data frame 

colnames(pred_data)<-c("feature_film","drama","runtime","mpaa_rating_R","thtr_rel_year","oscar_season","summer_season","imdb_rating","imdb_num_votes","critics_score","best_pic_nom","best_pic_win","best_actor_win","best_actress_win","best_dir_win","top200_box")

#the next portion ensures that the factor columns have the correct levels and numeric data points are stored as numbers
lvls<-c("no","yes")
indx<-c(1,2,4,6,7,11:16)
pred_data[indx]<-lapply(pred_data[indx],factor,levels=lvls)

indx<-c(3,5,8:10)
pred_data[indx]<-lapply(pred_data[indx],function(x) as.numeric(as.character(x)))

str(pred_data)
```

Now, we will **make the prediction using Bayesian Model Averaging or BMA**. 

```{r}
BMA.new = predict(bma_score, newdata = pred_data, estimator = "BMA", se.fit = TRUE)
confint(BMA.new,parm="pred")

```

The model predicts that the movie has an audience score of 68 with a 95% probability that the score is between 48 and 87. The actual audience score of the movie on Rotten Tomatoes is 72%, so the prediction was not too far off. 

* * *

## Part 6: Conclusion

In the above research, we were able to construct 2^16 models using MCMC methods, and determine posterior inclusion probabilities of the variables. It showed us that `imdb_rating`, `critics_score` and `runtime` had the highest pips and the model with the highest posterior probability included only `imdb_rating`, `runtime`, and `critics_score`. We did not select any one model, but used Bayesian Model Averaging to determine coefficients and make predictions. We carried out model diagnostics and discovered that the residuals did not have constant variance, leading us to think that some linear assumptions were being violated. 

One of the **drawbacks of this modeling exercise was that we did not check for collinearity between variables**, which could influence the posterior inclusion probabilities. Additionally, we used variables like `imdb_rating` and `imdb_num_votes` as explanatory variables even though these attributes of movie popularity, not attributes of the movie itself. 

We also **did not remove any outliers from the data**, and this could be skewing the model. 

Since we saw potential existence of non-linear relationships in the residual plot, we **could have transformed some of the explanatory variables to logs or squares to capture linear relationships in non-linear context.**

We also discussed some of the biases in the data in the first section, giving rise to concerns related to generalizability. 

In the future, I would collect data on more movie attributes such as the marketing budget, production value, original plot vs remake, languages, etc., and also expand the time frame for which I am collecting data. I would remove collinear variables and outliers. I would focus on movie attributes only, removing variables like `imdb_rating` and `imdb_num_votes`. 